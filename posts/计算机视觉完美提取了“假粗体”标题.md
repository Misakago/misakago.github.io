1. # 计算机视觉完美提取了“假粗体”标题

1. ### 问题背景：一只“看不见”的拦路虎

最近接到一个看似简单的需求文档完整性校验：**从一份 PDF 文档中自动提取所有章节标题。**

按照以往的经验，这属于常规操作。通常 PDF 中的标题会有特定的层级标签，或者字体会有明显的 `Bold`（加粗）属性。然而，在处理这份特定的中文文档时，我们撞上了一堵墙：

- **视觉上：** 标题明明比正文粗，肉眼区分一目了然。
- **数据上：** 标题和正文使用的是**完全相同**的字体文件，字号也完全一致。
- **元数据：** 使用 `PyMuPDF` (fitz) 查看，字体的 `flags` 并没有 `bold` 属性。

这是一种典型的“假粗体”（Fake Bold）**。生成 PDF 的软件（如 Word 或某些排版引擎）为了模拟粗体效果，并没有切换字重，而是通过**多次微移重印**或**描边渲染来实现变粗。

这就导致了一个无解的局面：**在代码眼里，标题和正文是孪生兄弟，完全无法区分。**

1. ### 那些走不通的弯路

为了解决这个问题，我们尝试了几乎所有常规和非常规手段，但都铩羽而归：

1. **OCR** **+** **LLM****（****大模型****）：** 将页面转图，用 OCR 识别文字，再丢给 LLM 提取。
   1. *失败原因：* 速度极慢，且 LLM 经常“幻觉”，对于层级结构的还原很不稳定，成本也高。
2. **PDF 转 Word/****HTML****：** 寄希望于转换工具能识别粗体。
   1. *失败原因：* 转换后的文档同样丢失了 `<b>` 标签，样式变成了一堆乱七八糟的 `span`，毫无规律。
3. **字号分析：**
   1. *失败原因：* 绝望地发现，这份文档的各级标题和正文竟然是**一样大的**。
4. **黑度占比（****Pixel** **Density）：** 计算文字区域黑色像素的百分比。
   1. *失败原因：* 复杂的汉字（如“繁”）天生就比简单的汉字（如“一”）黑度高。这会导致笔画多的正文被误判为标题。

1. ### 破局思路：像人眼一样“看”文档

既然元数据在撒谎，那就回到最原始的逻辑：**为什么人眼能一眼看出它是标题？**

因为**笔画变粗了**。

无论汉字结构多么复杂，只要它是粗体，其**笔画的物理宽度**（Stroke Width）一定大于细体。我们们需要一个算法，能够忽略汉字的复杂结构，直接度量“笔画的厚度”。

**解决方案：PyMuPDF（用于操作 PDF） + OpenCV（用于视觉分析） + K-Means（用于****聚类****判定）。**

1. ### 核心技术原理与演进

#### 4.1 核心算法：距离变换（Distance Transform）

直接计算黑色像素占比行不通，我们们采用 `cv2.distanceTransform`。 这个算法计算图像中**每个前景像素（白色）到最近背景像素（黑色）的距离**。

- 在笔画的边缘，距离接近 0。
- 在笔画的中心线（骨架），距离最大。
- **这个最大的距离值，就约等于笔画半径。**

通过计算一个汉字区域内所有像素的距离平均值，我们们就能得到一个不受汉字结构影响的“**视觉粗细分（Thickness Score）**”。

#### 4.2 难点一：标点符号与英文的干扰

一开始，我们直接对整行文字进行分析，结果惨不忍睹。

- 英文单词的笔画逻辑与中文不同。
- **标点符号（如逗号、分号）是极其密集的色块**。一个极小的 `.` 或 `；` 在二值化后密度极高，会瞬间拉高整行的“粗细分”，导致正文里的短句被误判为标题。

**对策：只看汉字（在此案例中，标题一定是中文）。** 利用正则 `[\u4e00-\u9fff]` 过滤，只有汉字才参与计算。

#### 4.3 难点二：物理粘连（The "Redaction" Trick）

虽然我们在代码逻辑上过滤了标点，但在 OpenCV 切割图片（ROI）时，如果一个逗号紧挨着汉字，切割出来的矩形框里难免会带入逗号的黑色像素，依然会干扰计算。

**终极杀招：物理擦除（Redaction）。**

既然标点碍事，我们就在渲染图片给 OpenCV 之前，**直接在 PDF 层面把非汉字内容“涂白”**。

1. 利用 PyMuPDF 获取所有非汉字字符的坐标。
2. 使用 `page.add_redact_annot()` 对这些区域添加白色遮罩。
3. `page.apply_redactions()` 应用擦除。
4. **渲染出一张只有汉字悬浮在空中的“纯净图片”。**

这样，OpenCV 看到的只有纯粹的汉字，准确率瞬间提升至 99%。

#### 4.4 难点三：无监督聚类

不同扫描件的分辨率不同，我们们不能硬编码 `score > 1.5` 这种阈值。 我们使用了 **K-Means** **聚类算法**。将全书所有行的粗细分丢进去，让算法自动分成“粗体组”和“细体组”。即使正文是 0.8，标题是 1.2，算法也能精准切分。

#### 4.5 难点四：丢失的序号（Corner Case）

由于我们们“擦除”了数字，提取出来的标题变成了 `" 目的与范围"`，原本的 `"1 目的与范围"` 中的 `"1"` 丢失了。或者有时候序号和标题在 PDF 结构里甚至是分行的。

**对策：空间回溯（Spatial Recovery）。** 确定某行汉字是标题后，利用它的坐标（BBox），去原始文档（未擦除版）中查找“位于该行左侧”**且**“高度对齐”的字符，将其召回并拼接。

1. ### 最终代码实现（精华版）

这是集成了上述所有智慧的最终逻辑摘要：

```Python
import fitz  # PyMuPDFimport cv2
import numpy as np
from sklearn.cluster import KMeans
import re

def analyze_pdf_titles(pdf_path):# 双句柄模式：doc用于擦除渲染，doc_orig用于回溯文本
    doc = fitz.open(pdf_path)
    doc_orig = fitz.open(pdf_path)
    
    all_lines = []

    for page_num, page in enumerate(doc):
        # 1. 【物理擦除】准备工作：找出所有非汉字
        text_dict = page.get_text("rawdict")
        redact_list = []
        for block in text_dict["blocks"]:
            for line in block["lines"]:
                for span in line["spans"]:
                    for char in span["chars"]:
                        # 如果不是汉字 (U+4E00 ~ U+9FFF)if not ('\u4e00' <= char["c"] <= '\u9fff'):
                            redact_list.append(char["bbox"])

        # 2. 【物理擦除】执行：将非汉字涂成白色for bbox in redact_list:
            page.add_redact_annot(bbox, fill=(1, 1, 1))
        page.apply_redactions()

        # 3. 【视觉分析】渲染高清图 -> 转灰度 -> 二值化
        pix = page.get_pixmap(matrix=fitz.Matrix(3, 3), alpha=False)
        img = np.frombuffer(pix.samples, dtype=np.uint8).reshape(pix.h, pix.w, 3)
        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        # 反转：文字变白，背景变黑
        _, img_bin = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)

        # 4. 【计算粗细】针对剩下的纯汉字计算距离变换# ... (遍历 text_dict 计算每个汉字的 stroke width) ...# 核心算法：# dist = cv2.distanceTransform(roi, cv2.DIST_L2, 3)# thickness = np.mean(dist[roi > 0])# 收集每行的 thickness_score# 5. 【聚类判定】K-Means 自动区分粗细
    scores = np.array([x['score'] for x in all_lines]).reshape(-1, 1)
    kmeans = KMeans(n_clusters=2).fit(scores)
    bold_label = np.argmax(kmeans.cluster_centers_) # 中心值大的那一类是标题# 6. 【结果输出与回溯】
    results = []
    for line, label in zip(all_lines, kmeans.labels_):
        if label == bold_label:
            # 过滤掉伪标题（如 "见；"）if not is_valid_title(line['text']): continue# 【空间回溯】去 doc_orig 找回左边丢失的序号 "1. "
            full_text = recover_prefix(doc_orig[line['page']], line['bbox'], line['text'])
            results.append(full_text)
            
    return results
```

1. ### 总结与启示

这次经历给我们最大的启示是：**不要被数据的表象（Metadata）所限制，要善用“多模态”思维。**

当文本解析（NLP）走进死胡同时，不妨切换视角，用计算机视觉（CV）去“看”文档。

- **Metadata 说它是细体，但****像素****说它是粗体，像素不会撒谎。**
- **“先擦除再识别”** 的思路，有效地解决了噪点干扰问题，这种“减法思维”在处理非结构化数据时极其有效。

这套方案最终在我们的项目中实现了 **100% 的提取准确率**，且处理几百页的 PDF 仅需数秒，远超 OCR 方案。