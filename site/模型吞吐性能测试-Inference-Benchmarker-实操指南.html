<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>模型吞吐性能测试&Inference Benchmarker 实操指南 - Misaka's Tech Blog</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
        }
        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        header {
            background: #2c3e50;
            color: white;
            padding: 2rem 0;
            margin-bottom: 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        header h1 { margin: 0; font-size: 2rem; }
        header p { opacity: 0.8; margin-top: 0.5rem; }
        nav {
            background: #34495e;
            padding: 1rem 0;
            position: sticky;
            top: 0;
            z-index: 100;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        nav a {
            color: #ecf0f1;
            text-decoration: none;
            padding: 0.5rem 1rem;
            margin: 0 0.5rem;
            border-radius: 4px;
            transition: background 0.3s;
        }
        nav a:hover { background: #1abc9c; }
        .article {
            background: white;
            padding: 2rem;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            margin-bottom: 2rem;
        }
        .article h1 { color: #2c3e50; margin-bottom: 1rem; font-size: 2rem; }
        .article h2 { color: #34495e; margin: 2rem 0 1rem; font-size: 1.5rem; border-bottom: 2px solid #ecf0f1; padding-bottom: 0.5rem; }
        .article h3 { color: #7f8c8d; margin: 1.5rem 0 0.5rem; font-size: 1.3rem; }
        .article h4 { color: #95a5a6; margin: 1rem 0 0.5rem; font-size: 1.1rem; }
        .article p { margin-bottom: 1rem; text-align: justify; }
        .article ul, .article ol { margin-left: 2rem; margin-bottom: 1rem; }
        .article li { margin-bottom: 0.5rem; }
        .article code {
            background: #f8f9fa;
            padding: 0.2rem 0.4rem;
            border-radius: 3px;
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, monospace;
            font-size: 0.9em;
        }
        .article pre {
            background: #2c3e50;
            color: #ecf0f1;
            padding: 1.5rem;
            border-radius: 6px;
            overflow-x: auto;
            margin-bottom: 1.5rem;
        }
        .article pre code {
            background: transparent;
            padding: 0;
            color: inherit;
        }
        .article blockquote {
            border-left: 4px solid #3498db;
            padding-left: 1.5rem;
            margin: 1.5rem 0;
            color: #7f8c8d;
            font-style: italic;
        }
        .article table {
            width: 100%;
            border-collapse: collapse;
            margin: 1.5rem 0;
        }
        .article table th, .article table td {
            border: 1px solid #ddd;
            padding: 0.75rem;
            text-align: left;
        }
        .article table th { background: #34495e; color: white; }
        .article table tr:nth-child(even) { background: #f8f9fa; }
        .article img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            margin: 1.5rem 0;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        .article a { color: #3498db; text-decoration: none; }
        .article a:hover { text-decoration: underline; }
        .meta {
            color: #7f8c8d;
            font-size: 0.9rem;
            margin-bottom: 2rem;
            padding-bottom: 1rem;
            border-bottom: 1px solid #ecf0f1;
        }
        .index-item {
            background: white;
            padding: 1.5rem;
            border-radius: 8px;
            margin-bottom: 1rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: transform 0.3s, box-shadow 0.3s;
        }
        .index-item:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        .index-item h2 { margin-bottom: 0.5rem; }
        .index-item h2 a { color: #2c3e50; text-decoration: none; }
        .index-item h2 a:hover { color: #3498db; }
        .index-item p { color: #7f8c8d; }
        .index-date { color: #95a5a6; font-size: 0.9rem; }
        footer {
            text-align: center;
            padding: 2rem;
            color: #7f8c8d;
            margin-top: 3rem;
        }
        footer a { color: #3498db; text-decoration: none; }
        .back-link { display: inline-block; margin-bottom: 1rem; color: #3498db; text-decoration: none; }
        .tag { display: inline-block; background: #ecf0f1; padding: 0.25rem 0.75rem; border-radius: 20px; font-size: 0.85rem; margin-right: 0.5rem; }
        @media (max-width: 768px) {
            .container { padding: 10px; }
            .article { padding: 1.5rem; }
            header h1 { font-size: 1.5rem; }
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Misaka's Tech Blog</h1>
            <p>技术分享与实践记录</p>
        </div>
    </header>
    <nav>
        <div class="container">
            <a href="https://misakago.github.io/">首页</a>
            <a href="https://github.com/Misakago" target="_blank">GitHub</a>
        </div>
    </nav>
    <div class="container">
        
        <a href="/" class="back-link">← 返回首页</a>
        <article class="article">
            <h1>模型吞吐性能测试&Inference Benchmarker 实操指南</h1>
            <div class="meta">发布于 2026-02-09</div>
            <h1>模型吞吐性能测试&Inference Benchmarker 实操指南</h1>
<br>
<h2>推荐使用GPUStack的[性能测试脚本](https://github.com/gpustack/gpustack/blob/main/benchmarks/benchmark_serving.py)</h2>
<br>
<p><strong>11/26日更新：Inference Benchmarker过于复杂，如果只是想简单测试下关键性能指标。</strong></p>
<br>
<pre><code class="language-bash">python benchmark_serving.py \
  --model openai/gpt-oss-20b \
  --embeddings \
  --num-requests 100 \
  --concurrency 10 \
  --server-url http://127.0.0.1:8080 \
  --api-key gpustack-key-123 \
  --json \
  --result-file emb.json
============ Serving Benchmark Result ============
Successful requests:                     1000
Benchmark duration (s):                  42.03
Total input tokens:                      215312
Total generated tokens:                  194171
Request throughput (req/s):              23.79
Output token throughput (tok/s):         4620.12
Peak output token throughput (tok/s):    7484.00
Peak concurrent requests:                1000.00
Total Token throughput (tok/s):          9743.28
---------------Time to First Token----------------
Mean TTFT (ms):                          13654.59
Median TTFT (ms):                        13063.93
P99 TTFT (ms):                           30306.72
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          48.57
Median TPOT (ms):                        45.15
P99 TPOT (ms):                           110.58
---------------Inter-token Latency----------------
Mean ITL (ms):                           41.95
Median ITL (ms):                         33.96
P99 ITL (ms):                            112.27
==================================================</code></pre>
<br>
<p>部署完模型后，建议使用<a href="https://github.com/huggingface/inference-benchmarker" target="_blank">inference-benchmarker</a>测试下性能上限，便于控制并发预期。</p>
<br>
<img src="/images/img_14_NTA5YTNmNGQ2OGN.jpg" alt="架构图">
<br>
<img src="/images/img_15_NTMxMjk2MTRhMWQ.jpg" alt="配置示例">
<br>
<img src="/images/img_27_YWUxNzIyNGU0MDg.jpg" alt="测试结果">
<br>
<h2>Inference Benchmarker</h2>
<br>
<h3>1. 合并后的维度/指标表</h3>
<br>
<table><thead><tr>
<th>指标类别</th>
<th>具体项（合并后）</th>
<th>说明</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>吞吐量</strong></th>
<th><code>requests/sec</code> (RPS), <code>tokens/sec</code></th>
<th>每秒成功请求数；每秒生成 token 数</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>延迟（分位数）</strong></th>
<th><code>p50</code>, <code>p90</code>, <code>p95</code>, <code>p99</code></th>
<th>端到端延迟（ms），反映典型与尾部延迟</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>延迟（流/首包）</strong></th>
<th><code>TTFT</code>, <code>Inter-Token latency</code>, <code>Time per token</code></th>
<th>首个 token 返回时间、token 间延时</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>成功率 / 错误率</strong></th>
<th><code>success_rate</code>, <code>error_rate</code>, <code>timeout_count</code></th>
<th>成功率、失败或超时比例</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>资源/稳定性信号</strong></th>
<th>GPU 利用率、显存、CPU、内存、重试次数</th>
<th>需外部监控（如 <code>nvidia-smi</code>、Prometheus）</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>配置上下文</strong></th>
<th>prompt 长度、decode 参数、batch size、并发 VUs、请求率、profile</th>
<th>测试输入与行为参数，影响结果可比性</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>测试阶段指标</strong></th>
<th><code>warmup</code> 时间、<code>step duration</code>、<code>detected_max_throughput</code></th>
<th>warmup 是否充分、单步耗时、自动探测的最大吞吐</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>输出统计</strong></th>
<th>平均值、标准差、原始请求时间线（JSON）</th>
<th>用于后续分析（延迟分布、时间序列图）</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>其它</strong></th>
<th><code>latencies histogram</code>, <code>tail latency trends</code>, <code>percentile over time</code></th>
<th>排查突发延迟或资源退化</th>
</tr>
</thead><tbody>
<br>
<blockquote>**提示：** JSON 字段名示例：`requests_per_second`, `tokens_per_second`, `p50_latency_ms`, `p99_latency_ms`, `error_rate`</blockquote>
<br>
<h3>2. 三种模式的可运行命令</h3>
<br>
<blockquote>**注意：** `--benchmark-kind` 与 `--profile` 互斥，不能同时使用。</blockquote>
<br>
<h4>A) 显式指定模式（推荐用于可控测试）</h4>
<br>
<p><strong>Sweep（自动扫 QPS）</strong></p>
<br>
<pre><code class="language-bash">RUST_LOG=warn ./target/release/inference-benchmarker \
  --tokenizer-name Qwen/Qwen3-VL-8B-Instruct \
  --model-name qwen3-vl-8b-instruct \
  --url http://localhost:8001 \
  --api-key &lt;YOUR_API_KEY&gt; \
  --benchmark-kind sweep \
  --num-rates 10 \
  --duration 120s \
  --warmup 30s \
  --dataset-file data.json \
  --decode-options "num_tokens=80,max_tokens=100,min_tokens=60,variance=10"</code></pre>
<br>
<p><strong>Rate（固定请求率）</strong></p>
<br>
<pre><code class="language-bash">RUST_LOG=warn ./target/release/inference-benchmarker \
  --tokenizer-name Qwen/Qwen3-VL-8B-Instruct \
  --model-name qwen3-vl-8b-instruct \
  --url http://localhost:8001 \
  --api-key &lt;YOUR_API_KEY&gt; \
  --benchmark-kind rate \
  --rates "5,10,25,50" \
  --duration 120s \
  --warmup 30s \
  --dataset-file data.json \
  --decode-options "num_tokens=80,max_tokens=100,min_tokens=60,variance=10"</code></pre>
<br>
<p><strong>Throughput（固定并发 VUs）</strong></p>
<br>
<pre><code class="language-bash">RUST_LOG=warn ./target/release/inference-benchmarker \
  --tokenizer-name Qwen/Qwen3-VL-8B-Instruct \
  --model-name qwen3-vl-8b-instruct \
  --url http://localhost:8001 \
  --api-key &lt;YOUR_API_KEY&gt; \
  --benchmark-kind throughput \
  --max-vus 64 \
  --duration 120s \
  --warmup 30s \
  --dataset-file data.json \
  --decode-options "num_tokens=80,max_tokens=100,min_tokens=60,variance=10"</code></pre>
<br>
<h4>B) 使用预设 Profile（适合快速验证）</h4>
<br>
<pre><code class="language-bash">RUST_LOG=warn ./target/release/inference-benchmarker \
  --tokenizer-name Qwen/Qwen3-VL-8B-Instruct \
  --model-name qwen3-vl-8b-instruct \
  --url http://localhost:8001 \
  --api-key &lt;YOUR_API_KEY&gt; \
  --profile chat \
  --duration 120s \
  --warmup 30s</code></pre>
<br>
<blockquote>**注意：** Profile 内部已定义测试逻辑（可能是 rate/sweep/throughput 中的一种），无法再指定 `--benchmark-kind`。</blockquote>
<br>
<h3>3. 关键机制说明</h3>
<br>
<h4>Sweep 的 QPS 范围如何决定？</h4>
<br>
<ul>
<li><strong>最低 QPS：</strong> 通常为 1 RPS 或 <code>detected_max / 10</code>，代表轻载。</li>
<li><strong>最高 QPS：</strong> 通过递增负载直到出现大量错误/超时/延迟飙升，取前一个稳定点作为上限。</li>
</ol>
<br>
<p><strong>控制建议：</strong></p>
<br>
<ul>
<li>若需精确控制范围，先用 <code>rate</code> 模式手动探测上限；</li>
<li>或修改源码/配置强制设置 <code>min_rate</code> / <code>max_rate</code>（若支持）。</li>
</ol>
<br>
<h4>Rate vs Throughput 的本质区别</h4>
<br>
<table><thead><tr>
<th>模式</th>
<th>控制变量</th>
<th>行为</th>
<th>适用场景</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>Rate</strong></th>
<th>固定 RPS</th>
<th>恒定请求到达率</th>
<th>模拟真实 API QPS，测延迟/错误</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>Throughput</strong></th>
<th>固定 VUs（并发）</th>
<th>每个 VU 循环发请求</th>
<th>测并发容量、资源占用</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th><strong>Sweep</strong></th>
<th>自动探测 max</th>
<th>扫描 min→max QPS 曲线</th>
<th>快速评估系统极限</th>
</tr>
</thead><tbody>
<br>
<blockquote>**选型口诀：**</blockquote>
<p>></p>
<blockquote>- "我要知道 100 RPS 下 p99 是多少" → 用 `rate`</blockquote>
<blockquote>- "我要知道 32 并发下 GPU 是否打满" → 用 `throughput`</blockquote>
<blockquote>- "我想自动找出最大吞吐" → 用 `sweep`</blockquote>
<br>
<h3>4. 推荐压力测试流程（实操步骤）</h3>
<br>
<h4>Step 1: 准备</h4>
<br>
<ul>
<li>固定 <code>prompt</code> 和 <code>decode</code> 长度（如 <code>num_tokens=80</code>）</li>
<li>开启系统监控：<code>nvidia-smi -l 1</code>, <code>htop</code>, Prometheus 等</li>
<li>确保后端日志级别为 <code>INFO</code> 或 <code>WARN</code></li>
</ol>
<br>
<h4>Step 2: Sanity Check（功能验证）</h4>
<br>
<pre><code class="language-bash">--benchmark-kind rate --rates "1" --duration 30s --warmup 10s</code></pre>
<br>
<p><strong>预期：</strong> error_rate ≈ 0%，TTFT 正常</p>
<br>
<h4>Step 3: 粗略探测上限</h4>
<br>
<pre><code class="language-bash">for vus in 4 8 16 32; do
  ./bench ... --benchmark-kind throughput --max-vus $vus --duration 60s
done</code></pre>
<br>
<p><strong>目标：</strong> 找到第一个出现 error 或延迟突增的 VU 数</p>
<br>
<h4>Step 4: Sweep 扫描完整曲线</h4>
<br>
<pre><code class="language-bash">--benchmark-kind sweep --num-rates 10 --duration 120s --warmup 30s</code></pre>
<br>
<p><strong>分析输出：</strong> 定位 <code>detected_max_throughput</code> 和 p99 拐点</p>
<br>
<h4>Step 5: 关键点细测（Rate 模式）</h4>
<br>
<pre><code class="language-bash"># 假设 detected_max = 100 RPS
--benchmark-kind rate --rates "10,25,50,75,90" --duration 300s --warmup 30s</code></pre>
<br>
<p><strong>重点记录：</strong> p95/p99、error_rate、tokens/sec</p>
<br>
<h4>Step 6: 并发验证（Throughput）</h4>
<br>
<pre><code class="language-bash">--benchmark-kind throughput --max-vus 32 --duration 600s</code></pre>
<br>
<p><strong>观察：</strong> GPU 显存是否稳定？有无 OOM？延迟是否漂移？</p>
<br>
<h4>Step 7: 长稳测试（可选）</h4>
<br>
<ul>
<li>在 75% max 吞吐下跑 1–4 小时</li>
<li><strong>监控：</strong> 内存泄漏、错误累积、延迟趋势</li>
</ol>
<br>
<h4>Step 8: 结果归档</h4>
<br>
<ul>
<li>保存所有 <code>.json</code> 输出</li>
<li>绘制三张核心图：</li>
</ol>
<p>1. 吞吐 vs p99 延迟</p>
<p>2. QPS vs error_rate</p>
<p>3. GPU 利用率/显存 vs 时间</p>
<br>
<h3>5. 常见错误处理</h3>
<br>
<h4>错误：`--benchmark-kind cannot be used with --profile`</h4>
<br>
<p><strong>原因：</strong> CLI 设计为互斥。</p>
<br>
<p><strong>解决方案：</strong></p>
<br>
<ul>
<li><strong>方案 A（推荐）：</strong> 不用 <code>--profile</code>，改用 <code>--dataset-file</code> + <code>--decode-options</code> 显式控制</li>
<li><strong>方案 B：</strong> 只用 <code>--profile</code>，接受其内置行为（查看 <code>profiles/</code> 目录了解细节）</li>
</ol>
<br>
<blockquote>**最佳实践：** 生产级压测一律使用显式参数（方案 A），确保可复现、可对比。Profile 仅用于快速 demo。</blockquote>
<br>
<h4>LLM 压测与 vLLM 调度常见误区</h4>
<br>
<p><strong>误区 1：RPS = 模型处理速度</strong></p>
<br>
<p>RPS 是施加的压力，不是模型的实际处理能力。模型吞吐受限于 GPU decode 速度、批处理能力和 KV cache 容量。超限会导致排队、超时或拒绝。</p>
<br>
<p><strong>误区 2：decode-options 强制输出长度</strong></p>
<br>
<p>min_tokens / max_tokens 是生成目标，非硬性限制。若 prompt 要求更多内容，模型仍会生成更长响应，压测工具不会干预。</p>
<br>
<p><strong>误区 3：vLLM 不会排队或容量无限</strong></p>
<br>
<p>vLLM 内部基于队列 + scheduler + continuous batching。高负载下会持续接收请求并排队，但延迟和显存消耗将急剧上升。</p>
<br>
<p><strong>误区 4：不设置 max_num_seqs / max_model_len / kv_cache_size 无影响</strong></p>
<br>
<p>默认配置倾向于最大化吞吐，易导致 OOM 或延迟雪崩。必须根据硬件显存和预期负载显式限制。</p>
<br>
<p><strong>误区 5：RPS 与 QPS 本质不同</strong></p>
<br>
<p>二者语义略有差异，但常可互换：</p>
<br>
<ul>
<li><strong>RPS：</strong> 客户端每秒发送请求数（输入压力）</li>
<li><strong>QPS：</strong> 服务端每秒成功处理请求数（实际产能）</li>
</ol>
<br>
<p><strong>误区 6：fixed RPS 与 fixed VUs 等效</strong></p>
<br>
<ul>
<li><strong>Fixed VUs（固定虚拟用户）：</strong> 用户循环发请求，实际 RPS 随延迟波动</li>
<li><strong>Fixed RPS：</strong> 严格控制每秒请求数，更稳定可控</li>
</ol>
<br>
<p>两者压测结果差异显著。</p>
<br>
<p><strong>误区 7：warmup 仅用于"预热几秒"</strong></p>
<br>
<p>Warmup 的核心作用是消除 JIT 编译、模型加载、KV cache 冷启动等一次性开销，确保压测数据反映稳态性能。</p>
<br>
<h4>LLM 压测核心 Mental Model（5 句）</h4>
<br>
<ol>
<li>RPS 是施压，QPS 是产能。</li>
<li>超出产能就排队，排队则延迟上升。</li>
<li>KV cache 决定显存上限，撑满即 OOM。</li>
<li>decode-options 影响 token 数，但不强制截断。</li>
<li>vLLM 本质是"请求排队 + token 调度"的吞吐引擎。</li>
</ol>
<br>
<h2>模型部署相关</h2>
<br>
<h3>模型加速方案</h3>
<br>
<p><strong>扩展 KV 缓存</strong></p>
<br>
<p>集成扩展 KV 缓存方案，显著降低首 Token 延迟（TTFT），在 vLLM 中使用 LMCache，在 SGLang 中使用Hicache。特别适用于长上下文和多轮对话推理。</p>
<br>
<p><strong>推测解码配置</strong></p>
<br>
<p>支持前沿推测解码算法（EAGLE3、MTP、N-gram），提供草稿模型下载与管理，降低用户使用负担。</p>
<br>
<img src="/images/img_30_YzgxNDRlZjk1NDQ.png" alt="推测解码配置">
<br>
<h3>模型推荐列表</h3>
<br>
<table><thead><tr>
<th>类型</th>
<th>型号</th>
<th>推荐理由</th>
<th>显存（vLLM、KV Cache）</th>
<th>备注</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th>LLM</th>
<th>Qwen/Qwen3-30B-A3B-Instruct（上下文开多少）</th>
<th>速度快，资源占用少，能力强，没有烦人的思考模式</th>
<th>72 GB （基于vllm运行，自动压缩后48G）</th>
<th>--enable-auto-tool-choice  --tool-call-parser=hermes</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th>GPT-OSS-20B</th>
<th>速度极快</th>
<th>20GB</th>
<th>速度极快</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th>VLM</th>
<th>Qwen/Qwen3-VL-8B-Instruct（图片语意理解）</th>
<th>资源占用较少少，模型能力强</th>
<th>36 GB</th>
<th>--enable-auto-tool-choice  --tool-call-parser=hermes</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th>MinerU2.5-2509-1.2B（仅OCR）PaddleOCR-VL（仅OCR）</th>
<th>速度超快，资源占用少，OCR效果稳定</th>
<th>6GB</th>
<th>优先考虑PaddleOCR-VL，更加稳定</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th>Embedding</th>
<th>BAAI/bge-m3</th>
<th>老牌模型，资源占用少。Embedding一般确定好了后面就不会换了。这种模型还是得稳定，一些刚出的Embedding没有经过社区验证精度容易出问题</th>
<th>8 GB</th>
<th>维度1024</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th>Rerank</th>
<th>BAAI/bge-reranker-v2-m3</th>
<th>同上</th>
<th>4 GB</th>
</tr>
</thead><tbody>
<table><thead><tr>
<th>文档解析</th>
<th>自研文档解析服务</th>
<th>一般一张4090就可以了</th>
<th>大纲解析只有一个worker，多任务走队列；视觉解析有多个worker，并发上限取决于VLM上限。知识库文档同步会触发长期大量文档解析任务，需要做兜底机制确保服务稳定运行</th>
</tr>
</thead><tbody>
<br>
<h3>常见问题</h3>
<br>
<p>在使用张量并行（Tensor Parallelism, TP）部署大模型时，<strong>注意力头数必须能被使用的 GPU 数量整除</strong>，这是主流框架（如 vLLM、Megatron、DeepSpeed）的硬性要求。原因和应对方式如下：</p>
<br>
<p><strong>为什么必须整除？</strong></p>
<br>
<ul>
<li>多头注意力机制中，每个注意力头是独立计算的。</li>
<li>张量并行会把同一层的 Q、K、V 权重按"头"切分到不同 GPU 上，每张卡负责处理完整且数量相等的一组头。</li>
<li>如果头数不能被 GPU 数整除，就无法均匀分配，会导致：</li>
</ol>
<p>- 某些 GPU 需要处理"半个头"或拼接不同头的数据；</p>
<p>- 通信模式变得复杂且不均衡；</p>
<p>- 框架直接报错，拒绝启动。</p>
<br>
<p><strong>举例：</strong> 32 个头的模型，只能用 1、2、4、8、16、32 张卡做张量并行；不能用 3、5、6、7 等卡数。</p>
<br>
        </article>
    
    </div>
    <footer>
        <p>&copy; 2026 Misaka. Powered by <a href="https://pages.github.com/">GitHub Pages</a></p>
        <p><a href="https://github.com/Misakago">https://github.com/Misakago</a></p>
    </footer>
</body>
</html>
